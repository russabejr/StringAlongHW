{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russabejr/StringAlongHW/blob/main/DDSP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2888672f",
      "metadata": {
        "id": "2888672f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e219b35-21cb-4984-f673-3084f7de7158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-08 20:15:46--  https://digitalmusicprocessing.github.io/HW6_StringAlong/data.wav\n",
            "Resolving digitalmusicprocessing.github.io (digitalmusicprocessing.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\n",
            "Connecting to digitalmusicprocessing.github.io (digitalmusicprocessing.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25135448 (24M) [audio/wav]\n",
            "Saving to: ‘data.wav.1’\n",
            "\n",
            "data.wav.1          100%[===================>]  23.97M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-08 20:15:53 (191 MB/s) - ‘data.wav.1’ saved [25135448/25135448]\n",
            "\n",
            "--2025-05-08 20:15:53--  https://digitalmusicprocessing.github.io/HW6_StringAlong/marvin.wav\n",
            "Resolving digitalmusicprocessing.github.io (digitalmusicprocessing.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\n",
            "Connecting to digitalmusicprocessing.github.io (digitalmusicprocessing.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 328174 (320K) [audio/wav]\n",
            "Saving to: ‘marvin.wav.1’\n",
            "\n",
            "marvin.wav.1        100%[===================>] 320.48K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-08 20:15:53 (10.2 MB/s) - ‘marvin.wav.1’ saved [328174/328174]\n",
            "\n",
            "--2025-05-08 20:15:53--  https://digitalmusicprocessing.github.io/HW6_StringAlong/adele.wav\n",
            "Resolving digitalmusicprocessing.github.io (digitalmusicprocessing.github.io)... 185.199.109.153, 185.199.108.153, 185.199.111.153, ...\n",
            "Connecting to digitalmusicprocessing.github.io (digitalmusicprocessing.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 633910 (619K) [audio/wav]\n",
            "Saving to: ‘adele.wav.1’\n",
            "\n",
            "adele.wav.1         100%[===================>] 619.05K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-08 20:15:54 (19.6 MB/s) - ‘adele.wav.1’ saved [633910/633910]\n",
            "\n",
            "Requirement already satisfied: torchcrepe in /usr/local/lib/python3.11/dist-packages (0.0.23)\n",
            "Requirement already satisfied: librosa>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (0.11.0)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (0.4.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (1.15.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchcrepe) (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.9.1->torchcrepe) (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchcrepe) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchcrepe) (1.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa>=0.9.1->torchcrepe) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa>=0.9.1->torchcrepe) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.9.1->torchcrepe) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.9.1->torchcrepe) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa>=0.9.1->torchcrepe) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa>=0.9.1->torchcrepe) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchcrepe) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa>=0.9.1->torchcrepe) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->torchcrepe) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->torchcrepe) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->torchcrepe) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa>=0.9.1->torchcrepe) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy import signal\n",
        "import librosa\n",
        "import time\n",
        "from scipy.io import wavfile\n",
        "\n",
        "!wget https://digitalmusicprocessing.github.io/HW6_StringAlong/data.wav\n",
        "!wget https://digitalmusicprocessing.github.io/HW6_StringAlong/marvin.wav\n",
        "!wget https://digitalmusicprocessing.github.io/HW6_StringAlong/adele.wav\n",
        "\n",
        "!pip install torchcrepe\n",
        "import torchcrepe # https://github.com/maxrmorrison/torchcrepe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c165d6",
      "metadata": {
        "id": "07c165d6"
      },
      "source": [
        "# Utility Functions (Given)\n",
        "\n",
        "General purpose functions that will help us with specific parts of the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3771cec9",
      "metadata": {
        "id": "3771cec9"
      },
      "outputs": [],
      "source": [
        "def upsample_time(X, hop_length, mode='nearest'):\n",
        "    \"\"\"\n",
        "    Upsample a tensor by a factor of hop_length along the time axis\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: torch.tensor(M, T, N)\n",
        "        A tensor in which the time axis is axis 1\n",
        "    hop_length: int\n",
        "        Upsample factor\n",
        "    mode: string\n",
        "        Mode of interpolation.  'nearest' by default to avoid artifacts\n",
        "        where notes in the violin jump by large intervals\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.tensor(M, T*hop_length, N)\n",
        "        Upsampled tensor\n",
        "    \"\"\"\n",
        "    X = X.permute(0, 2, 1)\n",
        "    X = nn.functional.interpolate(X, size=hop_length*X.shape[-1], mode=mode)\n",
        "    return X.permute(0, 2, 1)\n",
        "\n",
        "def fftconvolve(x, h):\n",
        "    \"\"\"\n",
        "    Perform a fast convolution of two tensors across their last axis\n",
        "    by using the FFT. Since the DFT assumes circularity, zeropad them\n",
        "    appropriately before doing the FFT and slice them down afterwards\n",
        "\n",
        "    The length of the result will be equivalent to np.convolve's 'same'\n",
        "\n",
        "    Refer to this module for more background:\n",
        "    https://ursinus-cs372-s2023.github.io/Modules/Module14/Video4\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: torch.tensor(..., N1)\n",
        "        First tensor\n",
        "    h: torch.tensor(..., N2)\n",
        "        Second tensor\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.tensor(..., max(N1, N2))\n",
        "    Tensor resulting from the convolution of x and y across their last axis,\n",
        "    \"\"\"\n",
        "    N = max(x.shape[-1], h.shape[-1])\n",
        "    if x.shape[-1] != h.shape[-1]:\n",
        "        # Zeropad so they're equal\n",
        "        if x.shape[-1] < N:\n",
        "            x = nn.functional.pad(x, (0, N-x.shape[-1]))\n",
        "        if h.shape[-1] < N:\n",
        "            h = nn.functional.pad(h, (0, N-h.shape[-1]))\n",
        "    x = nn.functional.pad(x, (0, N))\n",
        "    h = nn.functional.pad(h, (0, N))\n",
        "    X = torch.fft.rfft(x)\n",
        "    H = torch.fft.rfft(h)\n",
        "    y = torch.fft.irfft(X*H)\n",
        "    return y[..., 0:N]\n",
        "\n",
        "\n",
        "def plot_stft_comparison(F, L, X, Y, reverb, losses=torch.tensor([]), win=1024, sr=16000):\n",
        "    \"\"\"\n",
        "    Some code to help compare the STFTs of ground truth and output audio, while\n",
        "    also plotting the frequency, loudness, and reverb to get an idea of what the\n",
        "    inputs to the network were that gave rise to these ouputs.  It's very helpful\n",
        "    to call this method while monitoring the training of the network\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    F: torch.tensor(n_batches, n_samples/hop_length, 1)\n",
        "         Tensor holding the pitch estimates for the clips\n",
        "    L: torch.tensor(n_batches, n_samples/hop_length, 1)\n",
        "         Tensor holding the loudness estimates for the clips\n",
        "    X: torch.tensor(n_batches, n_samples, 1)\n",
        "        Ground truth audio\n",
        "    Y: torch.tensor(n_batches, n_samples, 1)\n",
        "        Output audio from the network->decoder\n",
        "    reverb: torch.tensor(reverb_len)\n",
        "        The learned reverb\n",
        "    losses: list\n",
        "        A list of losses over epochs over time\n",
        "    win: int\n",
        "        Window length to use in the STFT\n",
        "    sr: int\n",
        "        Sample rate of audio (used to help make proper units for time and frequency)\n",
        "    \"\"\"\n",
        "    hop = 256\n",
        "    hann = torch.hann_window(win).to(X)\n",
        "    SX = torch.abs(torch.stft(X.squeeze(), win, hop, win, hann, return_complex=True))\n",
        "    SY = torch.abs(torch.stft(Y.squeeze(), win, hop, win, hann, return_complex=True))\n",
        "    print(SX.shape)\n",
        "    extent = (0, SX.shape[2]*hop/sr, SX.shape[1]*sr/win, 0)\n",
        "    plt.subplot(321)\n",
        "    plt.imshow(torch.log10(SX.detach().cpu()[0, :, :]), aspect='auto', cmap='magma', extent=extent)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.ylim([0, 8000])\n",
        "    plt.xlabel(\"Time (Sec)\")\n",
        "    plt.ylabel(\"Frequency (hz)\")\n",
        "\n",
        "    plt.subplot(322)\n",
        "    plt.imshow(torch.log10(SY.detach().cpu()[0, :, :]), aspect='auto', cmap='magma', extent=extent)\n",
        "    plt.title(\"Synthesized\")\n",
        "    plt.ylim([0, 8000])\n",
        "    plt.xlabel(\"Time (Sec)\")\n",
        "    plt.ylabel(\"Frequency (hz)\")\n",
        "\n",
        "    plt.subplot(323)\n",
        "    plt.plot(F.detach().cpu()[0, :, 0])\n",
        "    plt.title(\"Fundamental Frequency\")\n",
        "    plt.xlabel(\"Window index\")\n",
        "    plt.ylabel(\"Hz\")\n",
        "    plt.subplot(324)\n",
        "    plt.plot(L.detach().cpu()[0, :, 0])\n",
        "    plt.title(\"Loudness\")\n",
        "    plt.xlabel(\"Window Index\")\n",
        "    plt.ylabel(\"Z-normalized dB\")\n",
        "    if torch.numel(losses) > 0:\n",
        "        plt.subplot(325)\n",
        "        plt.plot(losses.detach().cpu().numpy().flatten())\n",
        "        plt.yscale(\"log\")\n",
        "        plt.title(\"Losses (Current {:.3f})\".format(losses[-1]))\n",
        "        plt.xlabel(\"Epoch\")\n",
        "    plt.subplot(326)\n",
        "    plt.plot(reverb.detach().cpu().flatten())\n",
        "    plt.title(\"Impulse Response\")\n",
        "    plt.xlabel(\"Sample index\")\n",
        "\n",
        "################################################\n",
        "# Loudness code modified from original Google Magenta DDSP implementation in tensorflow\n",
        "# https://github.com/magenta/ddsp/blob/86c7a35f4f2ecf2e9bb45ee7094732b1afcebecd/ddsp/spectral_ops.py#L253\n",
        "# which, like this repository, is licensed under Apache2 by Google Magenta Group, 2020\n",
        "# Modifications by Chris Tralie, 2023\n",
        "\n",
        "def power_to_db(power, ref_db=0.0, range_db=80.0, use_tf=True):\n",
        "    \"\"\"Converts power from linear scale to decibels.\"\"\"\n",
        "    # Convert to decibels.\n",
        "    db = 10.0*np.log10(np.maximum(power, 10**(-range_db/10)))\n",
        "    # Set dynamic range.\n",
        "    db -= ref_db\n",
        "    db = np.maximum(db, -range_db)\n",
        "    return db\n",
        "\n",
        "def extract_loudness(x, sr, hop_length, n_fft=512):\n",
        "    \"\"\"\n",
        "    Extract the loudness in dB by using an A-weighting of the power spectrum\n",
        "    (section B.1 of the paper)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray(N)\n",
        "        Audio samples\n",
        "    sr: int\n",
        "        Sample rate (used to figure out frequencies for A-weighting)\n",
        "    hop_length: int\n",
        "        Hop length between loudness estimates\n",
        "    n_fft: int\n",
        "        Number of samples to use in each window\n",
        "    \"\"\"\n",
        "    # Computed centered STFT\n",
        "    S = librosa.stft(x, n_fft=n_fft, hop_length=hop_length, win_length=n_fft, center=True)\n",
        "\n",
        "    # Compute power spectrogram\n",
        "    amplitude = np.abs(S)\n",
        "    power = amplitude**2\n",
        "\n",
        "    # Perceptual weighting.\n",
        "    freqs = np.arange(S.shape[0])*sr/n_fft\n",
        "    a_weighting = librosa.A_weighting(freqs)[:, None]\n",
        "\n",
        "    # Perform weighting in linear scale, a_weighting given in decibels.\n",
        "    weighting = 10**(a_weighting/10)\n",
        "    power = power * weighting\n",
        "\n",
        "    # Average over frequencies (weighted power per a bin).\n",
        "    avg_power = np.mean(power, axis=0)\n",
        "    loudness = power_to_db(avg_power)\n",
        "    return np.array(loudness, dtype=np.float32)\n",
        "\n",
        "################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de4d056",
      "metadata": {
        "id": "7de4d056"
      },
      "source": [
        "# Part 1: Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae04bea",
      "metadata": {
        "id": "6ae04bea"
      },
      "source": [
        "## FM Synthesis Dataset (Given)\n",
        "\n",
        "For debugging, if you need it.  Your network should at least be able to learn these very simple sounds, so if it can't, you should figure out what the problem is before you move onto training your network on the real violin audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86ec3b8",
      "metadata": {
        "id": "f86ec3b8"
      },
      "outputs": [],
      "source": [
        "class FMDataset(Dataset):\n",
        "    def __init__(self, sr, hop_length, samples_per_batch=1000):\n",
        "        \"\"\"\n",
        "        Instantiate an fm dataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sr: int\n",
        "            Sample rate\n",
        "        hop_length: int\n",
        "            Samples between loudness and pitch frames\n",
        "        samples_per_batch: int\n",
        "            The length of this object\n",
        "        \"\"\"\n",
        "        self.sr = sr\n",
        "        self.hop_length = hop_length\n",
        "        self.samples_per_batch = samples_per_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.samples_per_batch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generate a random FM plucked string note between A3 and A5\n",
        "        over a duration of 4 seconds\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx: int\n",
        "            Index of example (ignored because data is random)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x: ndarray(sr*4)\n",
        "            Audio samples\n",
        "        pitch: ndarray(sr*4//hop_length)\n",
        "            The pitch (a constant line since this is one solid note)\n",
        "        loudness: ndarray(sr*4//hop_length)\n",
        "            Loudness\n",
        "        \"\"\"\n",
        "        note = np.random.randint(-12, 12)\n",
        "        sr = self.sr\n",
        "        ratio = 1\n",
        "        I = 8\n",
        "        lam = 3\n",
        "        duration = 4\n",
        "        envelope = lambda N, sr, lam: np.exp(-lam*np.arange(N)/sr)\n",
        "        N = int(duration*sr)\n",
        "        ts = np.arange(N)/sr\n",
        "        f = 440*2**(note/12)\n",
        "        fm = f*ratio\n",
        "        x = np.cos(2*np.pi*f*ts + envelope(N, sr, lam)*I*(np.cos(2*np.pi*fm*ts)))\n",
        "        loudness = envelope(N, sr, lam)\n",
        "        x = x*loudness\n",
        "        K = x.size//self.hop_length\n",
        "        loudness = np.array(loudness[0::self.hop_length], dtype=np.float32)\n",
        "        loudness = 10*np.log10(loudness**2+1e-8)\n",
        "        loudness = torch.from_numpy(loudness).view(K, 1)\n",
        "        x = torch.from_numpy(x).view(x.size, 1)\n",
        "        # Extract pitch and loudness\n",
        "        pitch = 440*(2**(note/12))*torch.ones(K)\n",
        "        pitch = pitch.view(K, 1)\n",
        "        return x, pitch, loudness\n",
        "\n",
        "## An example of a dataset\n",
        "sr = 16000\n",
        "dataset = FMDataset(sr, 160)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "(X, F, L) = next(iter(loader))\n",
        "ipd.Audio(X[0, :, 0], rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8eadec",
      "metadata": {
        "id": "db8eadec"
      },
      "source": [
        "## Instrument Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f8a8db",
      "metadata": {
        "id": "30f8a8db"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!\n",
        "class InstrumentData(Dataset):\n",
        "    def __init__(self, x, sr, hop_length, samples_per_batch=5000):\n",
        "        #1\n",
        "        self.sr = sr\n",
        "        self.x = x\n",
        "        self.hop_length = hop_length\n",
        "        self.samples_per_batch = samples_per_batch\n",
        "\n",
        "        #2\n",
        "        loudness = extract_loudness(x)\n",
        "        self.loudness_mean = loudness.mean()\n",
        "        self.loudness_stand_dev = loudness.std()\n",
        "        loudness = (loudness - self.loudness_mean) / self.loudness_stand_dev\n",
        "        self.loudness = loudness\n",
        "\n",
        "        #3\n",
        "        device = 'cuda'\n",
        "        pitch = torchcrepe.predict(torch.from_numpy(x).view((1, x.size)),sr,hop_length,50,2000,'full',batch_size=2048,device=device).flatten()\n",
        "        self.pitch = pitch\n",
        "\n",
        "        #4\n",
        "        min_len = min(len(self.loudness), len(self.pitch))\n",
        "        self.loudness = self.loudness[:min_len]\n",
        "        self.pitch = self.pitch[:min_len]\n",
        "        self.x = self.x[:min_len * hop_length]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.samples_per_batch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        clip_length = 4*self.sr\n",
        "\n",
        "        start_sample = np.random.randint(0, len(self.x) - clip_length)\n",
        "        end_sample = start_sample + clip_length\n",
        "\n",
        "        # Extract audio clip\n",
        "        x = torch.from_numpy(self.x[start_sample:end_sample]).view(x.size, 1)\n",
        "\n",
        "        start_frame = start_sample // self.hop_length\n",
        "        end_frame = end_sample // self.hop_length\n",
        "        pitch = torch.from_numpy(self.pitch[start_frame:end_frame]).view(x.size, 1)\n",
        "        loudness = torch.from_numpy(self.loudness[start_frame:end_frame]).view(x.size, 1)\n",
        "        return x, pitch, loudness"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5fbd3dc",
      "metadata": {
        "id": "c5fbd3dc"
      },
      "source": [
        "# Part 2a: Decoder Architecture\n",
        "\n",
        "Section B.2 of the paper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4f44df89",
      "metadata": {
        "id": "4f44df89"
      },
      "outputs": [],
      "source": [
        "def modified_sigmoid(x):\n",
        "    return 2*torch.sigmoid(x)**np.log(10) + 1e-7\n",
        "\n",
        "## TODO: Fill this in!\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.sequential = nn.Sequential(\n",
        "            nn.Linear(input_size, output_size),\n",
        "            nn.LayerNorm(output_size),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(input_size, output_size),\n",
        "            nn.LayerNorm(output_size),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(input_size, output_size),\n",
        "            nn.LayerNorm(output_size),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "class DDSPDecoder(nn.Module):\n",
        "    def __init__(self, n_units, sr, n_harmonics, n_bands, reverb_len):\n",
        "        super(DDSPDecoder, self).__init__()\n",
        "        self.pitch_layer = MLP(1, n_units)\n",
        "        self.loudness_layer = MLP(1, n_units)\n",
        "        self.gru = nn.GRU(input_size=n_units*2, hidden_size=n_units, batch_first=True)\n",
        "        self.joint_layer = MLP(3*n_units, n_units)\n",
        "\n",
        "        self.harmonics_decoder = nn.Linear(n_units, n_harmonics)\n",
        "        self.amplitude_decoder = nn.Linear(n_units, 1)\n",
        "        self.sub_filt_decoder = nn.Linear(n_units, n_bands)\n",
        "\n",
        "        self.reverb = nn.Parameter(torch.rand(reverb_len)*1e-4-0.5e-4)\n",
        "\n",
        "    def forward(self, F, L):\n",
        "        F_encoded = self.pitch_layer(F)\n",
        "        L_encoded = self.loudness_layer(L)\n",
        "\n",
        "        FL_cat_2 = torch.cat([F_encoded, L_encoded], dim=2)\n",
        "        GRU_output = self.gru(FL_cat_2)[0]\n",
        "\n",
        "        joint_input = torch.cat([F_encoded, L_encoded, GRU_output], dim=2)\n",
        "        joint_output = self.joint_layer(joint_input)\n",
        "\n",
        "        C = self.harmonics_decoder(joint_output)\n",
        "        A = self.amplitude_decoder(joint_output)\n",
        "        S = self.sub_filt_decoder(joint_output)\n",
        "\n",
        "        C = modified_sigmoid(C)\n",
        "        A = modified_sigmoid(A)\n",
        "        S = modified_sigmoid(S)\n",
        "\n",
        "        C = C/(1e-8+torch.sum(C, axis=2, keepdim=True))\n",
        "\n",
        "        reverb = torch.tanh(self.reverb)\n",
        "\n",
        "        return A, C, S, reverb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d9af7e3",
      "metadata": {
        "id": "0d9af7e3"
      },
      "source": [
        "# Part 2b: Synthesizer\n",
        "Section 3.2, 3.3 B.5\n",
        "\n",
        "Use the outputs of the decoder network to create audio samples, *using only torch methods*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e8b320",
      "metadata": {
        "id": "f0e8b320"
      },
      "source": [
        "## Subtractive Synthesizer (Given)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c013c9fc",
      "metadata": {
        "id": "c013c9fc"
      },
      "outputs": [],
      "source": [
        "def subtractive_synthesis(S, hop_length):\n",
        "    \"\"\"\n",
        "    Perform subtractive synthesis by converting frequency domain transfer\n",
        "    functions into causal, zero-phase, windowed impulse responses\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    S: n_batches x time x n_bands\n",
        "        Subtractive synthesis parameters\n",
        "    hop_length: int\n",
        "        Hop length between subtractive synthesis windows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.tensor(n_batches, time*hop_length, 1)\n",
        "        Subtractive synthesis audio components for each clip\n",
        "    \"\"\"\n",
        "\n",
        "    # Put an imaginary component of all 0s across a new last axis\n",
        "    # https://pytorch.org/docs/stable/generated/torch.view_as_complex.html\n",
        "    S = torch.stack([S, torch.zeros_like(S)], -1)\n",
        "    S = torch.view_as_complex(S)\n",
        "    # Do the inverse real DFT (assuming symmetry)\n",
        "    h = torch.fft.irfft(S)\n",
        "\n",
        "    # Shift the impulse response to zero-phase\n",
        "    nh = h.shape[-1]\n",
        "    h = torch.roll(h, nh//2, -1)\n",
        "    # Apply hann window\n",
        "    h = h*torch.hann_window(nh, dtype=h.dtype, device=h.device)\n",
        "    # Shift back to causal\n",
        "    h = nn.functional.pad(h, (0, hop_length-nh))\n",
        "    h = torch.roll(h, -nh//2, -1)\n",
        "\n",
        "    # Apply the impulse response to random noise in [-1, 1]\n",
        "    noise = torch.rand(h.shape[0],h.shape[1],hop_length).to(h.device)\n",
        "    noise = noise*2 - 1\n",
        "    noise = fftconvolve(noise, h).contiguous()\n",
        "\n",
        "    # Flatten nonoverlapping samples to one contiguous stream\n",
        "    return noise.reshape(noise.shape[0], noise.shape[1]*noise.shape[2], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "214f6cab",
      "metadata": {
        "id": "214f6cab"
      },
      "source": [
        "## Additive Synthesizer / Putting It Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a141f30d",
      "metadata": {
        "id": "a141f30d"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!\n",
        "def synthesize(self, F, C, A, S, reverb):\n",
        "    C_upsampled = self.upsample_time(C)\n",
        "    A_upsampled = self.upsample_time(A)\n",
        "    F_upsampled = self.upsample_time(F)\n",
        "\n",
        "    # Harmonics\n",
        "    batch_size, audio_time, n_harmonics = C_upsampled.shape\n",
        "    harmonic_numbers = torch.arange(1, n_harmonics + 1).view(1, 1, -1)\n",
        "    harmonic_freqs = harmonic_numbers * F_upsampled\n",
        "\n",
        "    phase = 2*torch.pi*torch.cumsum(harmonic_freqs / self.sr, dim=1)\n",
        "\n",
        "    harmonics = torch.sin(phase) * C_upsampled\n",
        "\n",
        "    additive = torch.sum(harmonics, dim=2) * A_upsampled\n",
        "\n",
        "    subtractive = self.subtractive_synthesis(S, self.hop_length)\n",
        "\n",
        "    Y = additive + subtractive\n",
        "\n",
        "    Y = self.fftconvolve(Y.squeeze(-1), reverb.view(1, -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b4b192",
      "metadata": {
        "id": "33b4b192"
      },
      "source": [
        "# Part 3: Loss Function\n",
        "\n",
        "Implement Multi-Scale Spectral Loss (DDSP Section 4.2.1)\n",
        "\n",
        "Use torch.stft to help you.  Don't forget to squeeze() the input to the STFT to get rid of the singleton dimension at the end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278a3c83",
      "metadata": {
        "id": "278a3c83"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140e4e7b",
      "metadata": {
        "id": "140e4e7b"
      },
      "source": [
        "# Part 4: Testing Example Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5a333d",
      "metadata": {
        "id": "5d5a333d"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088bdb2b",
      "metadata": {
        "id": "088bdb2b"
      },
      "source": [
        "# Part 5: Train Loop\n",
        "\n",
        "Put it all together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c82ef51",
      "metadata": {
        "scrolled": true,
        "id": "6c82ef51"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e68267",
      "metadata": {
        "id": "26e68267"
      },
      "source": [
        "# Musical Statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107571ef",
      "metadata": {
        "id": "107571ef"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!  Have fun!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}